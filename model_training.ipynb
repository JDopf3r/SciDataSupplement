{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning of compressed latent representation with autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './Models/'\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of the successful architectures were based on tanh activations only.\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, latent_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, input_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder[0](x)\n",
    "        activation1 = self.encoder[1](encoded)\n",
    "        encoded = self.encoder[2](activation1)\n",
    "        activation2 = self.encoder[3](encoded)\n",
    "\n",
    "        decoded = self.decoder[0](activation2)\n",
    "        activation3 = self.decoder[1](decoded)\n",
    "        decoded = self.decoder[2](activation3)\n",
    "        return decoded, activation1, activation2, activation3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Fetch mean kinase domain embeddings and create a TensorDataset.\n",
    "distinct_domains_df = pd.read_parquet('./Data/distinct_kinase_domain_embeddings.parquet')\n",
    "linearized_mean_embeddings = np.array(distinct_domains_df['Linearized Mean Embedding'].tolist())\n",
    "\n",
    "mean_padded_tensor = torch.FloatTensor(linearized_mean_embeddings)\n",
    "\n",
    "dataset = TensorDataset(mean_padded_tensor, mean_padded_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "input_size = linearized_mean_embeddings[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization parameters and training loop.\n",
    "\n",
    "model = Autoencoder(input_size, 512)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "# Optional: monitoring training process in tensorboard.\n",
    "# writer = SummaryWriter(log_dir='./logs/autoencoder')\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, _) in enumerate(dataloader):\n",
    "        outputs, act1, act2, act3 = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log activations every 10 batches.\n",
    "        # if i % 10 == 0:\n",
    "        #     writer.add_histogram('Activations/encoder_layer1', act1, epoch * len(dataloader) + i)\n",
    "        #     writer.add_histogram('Activations/encoder_layer2', act2, epoch * len(dataloader) + i)\n",
    "        #     writer.add_histogram('Activations/decoder_layer1', act3, epoch * len(dataloader) + i)\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "#writer.close()\n",
    "torch.save(model.state_dict(), './Models/autoencoder_512_dims_tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent representation from small model shape: (586, 512)\n",
      "Latent representation from large model shape: (586, 768)\n"
     ]
    }
   ],
   "source": [
    "small_model = Autoencoder(input_size, 512)\n",
    "large_model = Autoencoder(input_size, 768)\n",
    "\n",
    "# Load the trained model weights\n",
    "small_model.load_state_dict(torch.load('./Models/autoencoder_512_dims_tanh'))\n",
    "large_model.load_state_dict(torch.load('./Models/autoencoder_768_dims_tanh'))\n",
    "\n",
    "small_model.eval()\n",
    "large_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    latent_small = small_model.encoder(mean_padded_tensor)\n",
    "    latent_large = large_model.encoder(mean_padded_tensor)\n",
    "\n",
    "# Convert latent tensors to NumPy arrays for further use\n",
    "latent_small_np = latent_small.cpu().numpy()\n",
    "latent_large_np = latent_large.cpu().numpy()\n",
    "\n",
    "distinct_domains_df['Latent Small Embedding'] = latent_small_np.tolist()\n",
    "distinct_domains_df['Latent Large Embedding'] = latent_large_np.tolist()\n",
    "\n",
    "print(f\"Latent representation from small model shape: {latent_small_np.shape}\")\n",
    "print(f\"Latent representation from large model shape: {latent_large_np.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the learned embeddings.\n",
    "distinct_domains_df['Latent Small Embedding'] = latent_small_np.tolist()\n",
    "distinct_domains_df['Latent Large Embedding'] = latent_large_np.tolist()\n",
    "\n",
    "distinct_domains_df.to_parquet('./Data/distinct_kinase_domain_embeddings_with_latents.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
