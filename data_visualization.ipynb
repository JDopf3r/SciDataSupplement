{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from Bio import SeqIO, SearchIO\n",
    "import umap\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proteome visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring file paths and parameters.\n",
    "\n",
    "per_protein_embeddings_path = './Data/protein_embeddings_uniprot.h5'\n",
    "\n",
    "proteome_dataframe_path = './Data/proteome.parquet'\n",
    "kinase_domains_dataframe_path = './Data/distinct_kinase_domain_embeddings.parquet'\n",
    "\n",
    "n_neighbors_values = [5, 15, 30, 45]\n",
    "min_dist_values = [0.01, 0.1, 0.5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions.\n",
    "\n",
    "def run_UMAP_grid_scan(embeddings, n_neighbors_params=n_neighbors_values, min_dist_params=min_dist_values):\n",
    "    \"\"\"\n",
    "    Perform grid scan for UMAP hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (list): List of per protein embedding vectors.\n",
    "        n_neighbors_params (list): List of inputs for the n_neighbors argument of UMAP.\n",
    "        min_dist_params (list): List of inputs for the min_dist argument of UMAP.\n",
    "        \n",
    "    Returns:\n",
    "        umap_results (dict): A dictionary with (n_neighbors, min_dist) tuples as keys and\n",
    "                             2D UMAP embeddings as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    umap_results = {}\n",
    "    for n_neighbors in n_neighbors_values:\n",
    "        for min_dist in min_dist_values:\n",
    "            umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, random_state=42)  # fixed seed for reproducibility\n",
    "            embedding_2d = umap_model.fit_transform(embeddings)\n",
    "\n",
    "            umap_results[(n_neighbors, min_dist)] = embedding_2d\n",
    "\n",
    "    return umap_results\n",
    "\n",
    "\n",
    "def add_colors_to_proteome_df(terms_or_keywords, proteome_df, term_type='Keywords'):\n",
    "    \"\"\"\n",
    "    Adds a color column to the proteome DataFrame based on matching GO terms or keywords.\n",
    "    \n",
    "    Args:\n",
    "        terms_or_keywords (list): List of GO terms or keywords to match.\n",
    "        proteome_df (pd.DataFrame): DataFrame containing protein information.\n",
    "        term_type (str): Column to search for terms/keywords ('GO_terms' or 'Keywords').\n",
    "        \n",
    "    Returns:\n",
    "        proteome_df (pd.DataFrame): The updated DataFrame with a 'Color' column.\n",
    "    \"\"\"\n",
    "\n",
    "    unique_terms = set(terms_or_keywords)\n",
    "    colors = px.colors.qualitative.Plotly\n",
    "    color_palette = {term: colors[i % len(colors)] for i, term in enumerate(unique_terms)}\n",
    "\n",
    "    default_color = 'lightgrey'\n",
    "\n",
    "    proteome_df['Color'] = default_color\n",
    "\n",
    "    for term in terms_or_keywords:\n",
    "        mask = proteome_df[term_type].astype(str).str.contains(term, case=False, na=False)\n",
    "        proteome_df.loc[mask, 'Color'] = color_palette[term]\n",
    "\n",
    "    return proteome_df\n",
    "\n",
    "\n",
    "def show_UMAP_grid_scan(umap_results, hover_names, color, marker_size, *params):\n",
    "    \"\"\"\n",
    "    Visualizes the grid scan of different UMAP model parameters using subplots.\n",
    "\n",
    "    Args:\n",
    "        umap_results (dict): A dictionary with (n_neighbors, min_dist) tuples as keys and\n",
    "                             2D UMAP embeddings as values.\n",
    "        hover_names (list): A list of names to display when hovering over points in the scatter plots.\n",
    "        color (str or None): A color specification to apply to the points (e.g., a list of colors or a column name).\n",
    "        marker_size (int): The size of the scatter plot markers.\n",
    "        *params (lists): Two lists specifying the UMAP parameters:\n",
    "                         - `params[0]`: List of `n_neighbors` values.\n",
    "                         - `params[1]`: List of `min_dist` values.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the UMAP projections as a grid of subplots.\n",
    "    \"\"\"\n",
    "        \n",
    "    fig = sp.make_subplots(\n",
    "        rows=len(params[0]), \n",
    "        cols=len(params[1]),\n",
    "        subplot_titles=[f\"n_neighbors={n}, min_dist={d}\" for n in params[0] for d in params[1]],\n",
    "        horizontal_spacing=0.1, vertical_spacing=0.1\n",
    "        )\n",
    "    for i, n_neighbors in enumerate(params[0]):\n",
    "        for j, min_dist in enumerate(params[1]):\n",
    "            embedding_2d = umap_results[(n_neighbors, min_dist)]\n",
    "            \n",
    "            if color:\n",
    "                scatter = px.scatter(\n",
    "                    x=embedding_2d[:, 0], \n",
    "                    y=embedding_2d[:, 1],\n",
    "                    hover_name=hover_names,\n",
    "                    color=color,\n",
    "                    labels={'x': 'UMAP 1', 'y': 'UMAP 2'}\n",
    "                )\n",
    "            else:\n",
    "                    scatter = px.scatter(\n",
    "                    x=embedding_2d[:, 0], \n",
    "                    y=embedding_2d[:, 1],\n",
    "                    hover_name=hover_names,\n",
    "                    labels={'x': 'UMAP 1', 'y': 'UMAP 2'}\n",
    "                )\n",
    "\n",
    "            scatter.update_traces(marker=dict(size=marker_size))\n",
    "\n",
    "            for trace in scatter['data']:\n",
    "                trace.showlegend = False\n",
    "                fig.add_trace(trace, row=i+1, col=j+1)\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=1600, \n",
    "        width=1600,\n",
    "        title_text=\"UMAP Projections with Different Hyperparameters\",\n",
    "        dragmode='zoom'\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_ids = []\n",
    "embeddings = []\n",
    "\n",
    "with h5py.File(per_protein_embeddings_path, \"r\") as file:\n",
    "    print(f\"Number of entries: {len(file.items())}\")\n",
    "    for sequence_id, embedding in file.items():\n",
    "        if sequence_id:\n",
    "            protein_ids.append(sequence_id)\n",
    "            embeddings.append(np.array(embedding))\n",
    "\n",
    "    assert len(file.items()) == len(embeddings)\n",
    "\n",
    "per_protein_embeddings = np.array(embeddings)\n",
    "id_to_embedding = dict(zip(protein_ids, per_protein_embeddings))\n",
    "\n",
    "# Map embeddings to DataFrame entries by 'Uniprot ID'\n",
    "proteome_df = pd.read_parquet(proteome_dataframe_path)\n",
    "proteome_df['embedding'] = proteome_df['Uniprot ID'].map(id_to_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_results = run_UMAP_grid_scan(proteome_df['embedding'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteome_df = add_colors_to_proteome_df(['Kinase', 'G-protein coupled receptor'], proteome_df, term_type='Keywords')\n",
    "show_UMAP_grid_scan(umap_results, proteome_df.GeneName.tolist(), proteome_df['Color'].tolist(), 2, n_neighbors_values, min_dist_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinome visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring file paths and parameters.\n",
    "\n",
    "proteome_fasta_path = './Data/UP000005640_9606.fasta'\n",
    "hmmerscan_results_path = './Data/proteome_hmmer_results.out'\n",
    "per_residue_embeddings_path = './Data/per-residue.h5'  # TODO download file form https://www.uniprot.org/help/downloads\n",
    "\n",
    "# List of kinase specific GO terms present in the mapping supplied by https://current.geneontology.org/ontology/external2go/pfam2go\n",
    "kinase_go_terms = [\n",
    "        'GO:0016301', 'GO:0004672', 'GO:0052742', \n",
    "        'GO:0004618', 'GO:0004743', 'GO:0004797', \n",
    "        'GO:0003872', 'GO:0004674', 'GO:0004797', \n",
    "        'GO:0004611', 'GO:0003848', 'GO:0003873',\n",
    "        'GO:0004417', 'GO:0043752', 'GO:0004550',\n",
    "        'GO:0004340', 'GO:0004673', 'GO:0004371',\n",
    "        'GO:0004594', 'GO:0004788', 'GO:0004631',\n",
    "        'GO:0008671', 'GO:0035299', 'GO:0008531',\n",
    "        'GO:0008772', 'GO:0004797', 'GO:0009029',\n",
    "        'GO:0004611', 'GO:0008887'\n",
    "        ]\n",
    "\n",
    "# List of kinase domains that do not serve are not directly responsible for the kinases catalytic activity.\n",
    "domains_to_exclude = [\n",
    "        'PF06414', 'PF12063', 'PF11629',\n",
    "        'PF08826', 'PF05191', 'PF02807',\n",
    "        'PF12202', 'PF00433', 'PF09036',\n",
    "        'PF12605', 'PF09202', 'PF11640',\n",
    "        'PF08926', 'PF02816', 'PF08064',\n",
    "        'PF02734', 'PF15785'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions.\n",
    "\n",
    "def parse_pfam_go_file(url=\"https://current.geneontology.org/ontology/external2go/pfam2go\"):\n",
    "    \"\"\"\n",
    "    Parses the PFAM to GO-term mapping file from a specified URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the PFAM-to-GO mapping file.\n",
    "                   Default is \"https://current.geneontology.org/ontology/external2go/pfam2go\".\n",
    "\n",
    "    Returns:\n",
    "        defaultdict: A dictionary mapping GO terms to sets of PFAM accessions.\n",
    "    \"\"\"\n",
    "    \n",
    "    go_to_pfam = defaultdict(set)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "    lines = response.text.splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"Pfam:\"):\n",
    "            pfam_part, go_part = line.strip().split(' > GO:')\n",
    "            pfam_accession = pfam_part.split()[0].replace(\"Pfam:\", \"\")\n",
    "            go_term = go_part.split(' ; ')[1]\n",
    "            go_to_pfam[go_term].add(pfam_accession)\n",
    "\n",
    "    return go_to_pfam\n",
    "\n",
    "\n",
    "def parse_hmmer_results(hmmer_results_file):\n",
    "    \"\"\"\n",
    "    Parses an HMMER domtab result file.\n",
    "\n",
    "    Args:\n",
    "        hmmer_results_file (str): Path to the HMMER domtab result file.\n",
    "\n",
    "    Returns:\n",
    "        iterator: An iterator of parsed HMMER search results.\n",
    "    \"\"\"\n",
    "\n",
    "    parsed_hmmer_file = SearchIO.parse(hmmer_results_file, \"hmmscan3-domtab\")\n",
    "    return parsed_hmmer_file\n",
    "\n",
    "\n",
    "def filter_domains_by_GO(parsed_hmmer_file, go_to_pfam, target_go_terms, exluded_domains):\n",
    "    \"\"\"\n",
    "    Filters the parsed HMMER file to extract domains associated with specific GO terms.\n",
    "\n",
    "    Args:\n",
    "        parsed_hmmer_file (iterator): Parsed HMMER search results.\n",
    "        go_to_pfam (dict): Dictionary mapping GO terms to PFAM accessions.\n",
    "        target_go_terms (list): List of GO terms to filter by.\n",
    "        exluded_domains (list): List of PFAM accessions to exclude from the results.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples containing filtered domain information:\n",
    "              (query_id, pfam_accession, env_start, env_end, query_start, query_end, target_description).\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_domains = []\n",
    "    for queryresult in parsed_hmmer_file:\n",
    "        for hit in queryresult.hits:\n",
    "            for hsp in hit.hsps:\n",
    "                pfam_accession = hit.accession.split('.')[0]\n",
    "                if hsp.evalue <= 1e-5:  # Threshold for domains with significant matches to a PFAM family.\n",
    "                    for go_term in target_go_terms:\n",
    "                        if (pfam_accession in go_to_pfam[go_term]) and (pfam_accession not in exluded_domains):\n",
    "                            target_description = hit.description\n",
    "                            filtered_domains.append((queryresult.id, pfam_accession, hsp.env_start, hsp.env_end, hsp.query_start, hsp.query_end, target_description))\n",
    "    return filtered_domains\n",
    "\n",
    "\n",
    "def parse_fasta_file(fasta_file):\n",
    "    \"\"\"\n",
    "    Parses a multi-FASTA file and creates a mapping of UniProt IDs to sequences.\n",
    "\n",
    "    Args:\n",
    "        fasta_file (str): Path to the FASTA file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping UniProt IDs to sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences = {}\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        sequences[record.id] = record.seq\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def extract_domain_sequences(filtered_domains, sequences):\n",
    "    \"\"\"\n",
    "    Extracts domain sequences from full-length protein sequences based on domain boundaries.\n",
    "\n",
    "    Args:\n",
    "        filtered_domains (list): A list of tuples containing filtered domain information:\n",
    "                                 (query_id, pfam_accession, env_start, env_end, query_start, query_end, target_description).\n",
    "        sequences (dict): A dictionary mapping UniProt IDs to full-length sequences.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples containing extracted domain sequences:\n",
    "              (query_id, protein_id, entry_name, pfam_accession, env_start, env_end,\n",
    "              query_start, query_end, domain_sequence, target_description).\n",
    "    \"\"\"\n",
    "    \n",
    "    extracted_sequences = []\n",
    "    for domain in filtered_domains:\n",
    "        query_id, pfam_accession, env_start, env_end, query_start, query_end, target_description = domain\n",
    "        sequence = sequences.get(query_id)\n",
    "        if sequence:\n",
    "            domain_seq = sequence[env_start:env_end]\n",
    "            extracted_sequences.append((query_id,query_id.split('|')[1],query_id.split('|')[2], pfam_accession, env_start, env_end, query_start, query_end, str(domain_seq), target_description))\n",
    "    return extracted_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_to_pfam_dict = parse_pfam_go_file()\n",
    "parsed_hmmer_results = parse_hmmer_results(hmmerscan_results_path)\n",
    "filtered_domains = filter_domains_by_GO(parsed_hmmer_results, go_to_pfam_dict, kinase_go_terms, domains_to_exclude)\n",
    "human_proteome_sequences = parse_fasta_file(proteome_fasta_path)\n",
    "extracted_sequences = extract_domain_sequences(filtered_domains, human_proteome_sequences)\n",
    "\n",
    "kinase_domain_df = pd.DataFrame(extracted_sequences, \n",
    "                                columns=\n",
    "                                    [\n",
    "                                    'Query ID','Uniprot ID','Protein', \n",
    "                                    'Pfam Accession', 'Env Start', 'Env End', \n",
    "                                    'Query Start', 'Query End', 'Domain Sequence', \n",
    "                                    'Domain Description'\n",
    "                                    ]\n",
    "                                )\n",
    "kinase_domain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To account for cases where a single domain is matched by multiple HMM models (resulting in multiple PFAM IDs), \n",
    "# this logic selectively retains non-overlapping domains.\n",
    "\n",
    "def has_overlap(domain1, domain2):\n",
    "    \"\"\"\n",
    "    Checks if two domains overlap based on their start and end positions.\n",
    "\n",
    "    Args:\n",
    "        domain1 (tuple): The start and end positions of the first domain (start1, end1).\n",
    "        domain2 (tuple): The start and end positions of the second domain (start2, end2).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the domains overlap, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    start1, end1 = domain1\n",
    "    start2, end2 = domain2\n",
    "    return max(start1, start2) <= min(end1, end2)\n",
    "\n",
    "\n",
    "def filter_nonoverlapping_domains(group):\n",
    "    \"\"\"\n",
    "    Filters out overlapping domains for each UniProt ID group.\n",
    "\n",
    "    Args:\n",
    "        group (DataFrame): A DataFrame containing domain information with columns 'Env Start' and 'Env End'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing non-overlapping domains.\n",
    "    \"\"\"\n",
    "\n",
    "    selected_rows = []\n",
    "    for idx, row in group.iterrows():\n",
    "        current_domain = (row['Env Start'], row['Env End'])\n",
    "        overlap = any(has_overlap(current_domain, (group.loc[i, 'Env Start'], group.loc[i, 'Env End'])) for i in selected_rows)\n",
    "        \n",
    "        if not overlap:\n",
    "            selected_rows.append(idx)\n",
    "\n",
    "    return group.loc[selected_rows]\n",
    "\n",
    "\n",
    "def crop_embedding(row):\n",
    "    \"\"\"\n",
    "    Crops the embedding of a protein domain based on specified domain boundaries.\n",
    "\n",
    "    Args:\n",
    "        row (Series): A pandas Series containing:\n",
    "                      - 'Uniprot ID' (str): The UniProt identifier.\n",
    "                      - 'Query Start' (int): Start position of the domain in the query sequence.\n",
    "                      - 'Query End' (int): End position of the domain in the query sequence.\n",
    "                      - 'Embedding' (ndarray): The embedding matrix (shape: k x 1024) for the sequence.\n",
    "\n",
    "    Returns:\n",
    "        ndarray or None: The cropped embedding matrix based on domain boundaries.\n",
    "                         Returns None if the embedding is missing.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the cropped embedding length doesn't match the expected domain length.\n",
    "    \"\"\"\n",
    "    \n",
    "    uniprot_id = row['Uniprot ID']\n",
    "    query_start = row['Query Start']\n",
    "    query_end = row['Query End']\n",
    "    embedding = row['Embedding']\n",
    "\n",
    "    if embedding is not None:\n",
    "        # Crop the embedding based on the domain boundaries\n",
    "        cropped_embedding = embedding[query_start:query_end+1, :]  # Crop rows, keep all columns (1024)\n",
    "\n",
    "        domain_length = query_end - query_start + 1\n",
    "        assert cropped_embedding.shape[0] == domain_length, (\n",
    "            f\"Length mismatch for {uniprot_id}: Domain length is {domain_length} \"\n",
    "            f\"but cropped embedding length is {cropped_embedding.shape[0]}\"\n",
    "        )\n",
    "        return cropped_embedding\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_domains_df = kinase_domain_df.groupby('Uniprot ID', group_keys=False).apply(filter_nonoverlapping_domains, include_groups=True)\n",
    "distinct_domains_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_domains_df['Domain Length'] = distinct_domains_df['Env End'] - distinct_domains_df['Env Start']\n",
    "\n",
    "fig = px.histogram(distinct_domains_df, x='Domain Length', nbins=100, title=\"Distribution of Kinase Domain Lengths\")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Domain Length\",\n",
    "    yaxis_title=\"Frequency\",\n",
    "    template=\"plotly_white\",\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map = {}\n",
    "\n",
    "with h5py.File(per_residue_embeddings_path, \"r\") as file:\n",
    "    print(f\"Number of entries: {len(file.items())}\")\n",
    "    for sequence_id, embedding in file.items():\n",
    "        embedding_map[sequence_id] = np.array(embedding)\n",
    "\n",
    "# Add embeddings to respective domains and crop them to fit the domain length.\n",
    "distinct_domains_df['Embedding'] = distinct_domains_df['Uniprot ID'].map(embedding_map)\n",
    "distinct_domains_df.dropna(subset=['Embedding'], inplace=True)\n",
    "distinct_domains_df['Cropped Embedding'] = distinct_domains_df.apply(crop_embedding, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to allow for the application of dimensionalty reduction algorithms, the domain embeddings have to be padded.\n",
    "# Each row of the embedding matrix represents a single residue with a vector of size 1024.\n",
    "\n",
    "def pad_with_zeros(row, max_length):\n",
    "    \"\"\"Pad domain embeddings with zero vectors of shape (1, 1024).\"\"\"\n",
    "\n",
    "    cropped_embedding = row['Cropped Embedding']\n",
    "\n",
    "    if cropped_embedding is not None:\n",
    "        padding_length = max_length - cropped_embedding.shape[0]\n",
    "        \n",
    "        if padding_length > 0:\n",
    "            padding = np.zeros((padding_length, 1024), dtype=np.float32)\n",
    "            padded_embedding = np.vstack([cropped_embedding.astype(np.float32), padding])\n",
    "        else:\n",
    "            padded_embedding = cropped_embedding.astype(np.float32)\n",
    "        \n",
    "        return padded_embedding.flatten()\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def pad_with_mean(row, max_length):\n",
    "    \"\"\"Pad domain embeddings with column-wise mean vectors of shape (1, 1024).\"\"\"\n",
    "\n",
    "    cropped_embedding = row['Cropped Embedding']\n",
    "\n",
    "    if cropped_embedding is not None:\n",
    "        padding_length = max_length - cropped_embedding.shape[0]\n",
    "\n",
    "        if padding_length > 0:\n",
    "            mean_vector = np.mean(cropped_embedding, axis=0).astype(np.float32)\n",
    "            padding = np.tile(mean_vector, (padding_length, 1))\n",
    "            padded_embedding = np.vstack([cropped_embedding.astype(np.float32), padding])\n",
    "        else:\n",
    "            padded_embedding = cropped_embedding.astype(np.float32)\n",
    "        \n",
    "        return padded_embedding.flatten()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_identifier(row):\n",
    "    protein = row['Protein'].replace('_HUMAN', '')\n",
    "    return f\"{protein}_{row['Pfam Accession']}_{row['Env Start']}-{row['Env End']}\"\n",
    "    \n",
    "\n",
    "max_length = max([embedding.shape[0] for embedding in distinct_domains_df['Cropped Embedding'] if embedding is not None])\n",
    "\n",
    "distinct_domains_df['Linearized Zeros Embedding'] = distinct_domains_df.apply(lambda row: pad_with_zeros(row, max_length), axis=1)\n",
    "distinct_domains_df['Linearized Mean Embedding'] = distinct_domains_df.apply(lambda row: pad_with_mean(row, max_length), axis=1)\n",
    "distinct_domains_df['Identifier'] = distinct_domains_df.apply(create_identifier, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe, if the autoencoder is run on another machine.\n",
    "distinct_domains_df.drop(axis=1, columns=['Embedding', 'Cropped Embedding'], inplace=True)\n",
    "distinct_domains_df.to_parquet('./Data/distinct_kinase_domain_embeddings.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoders can be trained on the linearized kinase domain embeddings now. Please refer to the 'model_training.ipynb' notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the learned embeddings.\n",
    "distinct_domains_df = pd.read_parquet('./Data/distinct_kinase_domain_embeddings_with_latents.parquet')\n",
    "merged_df = pd.merge(proteome_df, distinct_domains_df, on='Uniprot ID', how='inner')\n",
    "\n",
    "print(f\"Number of entries in the merged DataFrame: {len(merged_df)}\")\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify and color the domains.\n",
    "STK_terms = ['GO:0004674']\n",
    "TK_terms = ['GO:0004713', 'GO:0004714', 'GO:0007169']\n",
    "\n",
    "# Function to classify domain based on GO terms\n",
    "def classify_domain(go_terms):\n",
    "    if any(term in STK_terms for term in go_terms):\n",
    "        return 'STK'\n",
    "    elif any(term in TK_terms for term in go_terms):\n",
    "        return 'TK'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply classification to the DataFrame\n",
    "# Ensure GO_terms are all strings before splitting\n",
    "merged_df['GO_terms'] = merged_df['GO_terms'].apply(lambda x: ','.join(x) if isinstance(x, np.ndarray) else x)\n",
    "merged_df['GO_terms'] = merged_df['GO_terms'].astype(str)  # Convert all entries to strings\n",
    "\n",
    "# Now apply the split and classification\n",
    "merged_df['GO_terms'] = merged_df['GO_terms'].apply(lambda x: x.split(','))\n",
    "merged_df['Domain_Type'] = merged_df['GO_terms'].apply(classify_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_UMAP_on_multiple_embeddings(embedding_arrays, subplot_titles, n_neighbors=30, min_dist=0.75):\n",
    "    \"\"\"\n",
    "    Run UMAP on different embedding types and store the results with subplot titles as keys.\n",
    "\n",
    "    Parameters:\n",
    "    - embedding_arrays (list of np.ndarray): List of embedding arrays.\n",
    "    - subplot_titles (list of str): Titles for each embedding type.\n",
    "    - n_neighbors (int): UMAP n_neighbors parameter.\n",
    "    - min_dist (float): UMAP min_dist parameter.\n",
    "\n",
    "    Returns:\n",
    "    - umap_results (dict): Dictionary with subplot titles as keys and 2D UMAP embeddings as values.\n",
    "    \"\"\"\n",
    "\n",
    "    umap_results = {}\n",
    "    for embeddings, title in zip(embedding_arrays, subplot_titles):\n",
    "        umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, random_state=42)  # fixed seed for reproducibility\n",
    "        embedding_2d = umap_model.fit_transform(embeddings)\n",
    "        \n",
    "        # Store the result with the subplot title as the key\n",
    "        umap_results[title] = embedding_2d\n",
    "\n",
    "    return umap_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_embeddings_array = np.array(merged_df['Linearized Zeros Embedding'].tolist())\n",
    "mean_embeddings_array = np.array(merged_df['Linearized Mean Embedding'].tolist())\n",
    "small_latent_array = np.array(merged_df['Latent Small Embedding'].tolist())\n",
    "large_latent_array = np.array(merged_df['Latent Large Embedding'].tolist())\n",
    "\n",
    "embedding_arrays = [\n",
    "    zeros_embeddings_array, \n",
    "    mean_embeddings_array, \n",
    "    small_latent_array, \n",
    "    large_latent_array\n",
    "]\n",
    "\n",
    "subplot_titles = [\n",
    "    \"UMAP on Zeros Padding\", \n",
    "    \"UMAP on Mean Padding\", \n",
    "    \"UMAP on Small Latent (512)\", \n",
    "    \"UMAP on Large Latent (768)\"\n",
    "]\n",
    "\n",
    "umap_results = run_UMAP_on_multiple_embeddings(embedding_arrays, subplot_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sp.make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=subplot_titles,\n",
    "    horizontal_spacing=0.1,\n",
    "    vertical_spacing=0.15\n",
    ")\n",
    "\n",
    "for idx, key in enumerate(subplot_titles):\n",
    "\n",
    "    scatter = px.scatter(\n",
    "        x=umap_results[key][:, 0], \n",
    "        y=umap_results[key][:, 1],\n",
    "        color=merged_df['Domain_Type'],\n",
    "        hover_name=distinct_domains_df['Identifier'],\n",
    "        labels={'x': 'UMAP 1', 'y': 'UMAP 2'}\n",
    "    )\n",
    "    scatter.update_traces(marker=dict(size=4))\n",
    "    \n",
    "    # Determine the row and column for subplot placement\n",
    "    row = (idx // 2) + 1\n",
    "    col = (idx % 2) + 1\n",
    "    \n",
    "    for trace in scatter['data']:\n",
    "        trace.showlegend = False\n",
    "        fig.add_trace(trace, row=row, col=col)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800, \n",
    "    width=800,\n",
    "    title_text=\"UMAP Projections: Zeros, Mean, Small Latent (512), and Large Latent (768)\",\n",
    "    dragmode='zoom',\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        x=1.05,\n",
    "        y=0.5,\n",
    "        traceorder='normal',\n",
    "        font=dict(size=12),\n",
    "        borderwidth=2\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sp.make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=subplot_titles,\n",
    "    horizontal_spacing=0.1,\n",
    "    vertical_spacing=0.15\n",
    ")\n",
    "\n",
    "sorted_identifiers = sorted(merged_df['Identifier'].unique())\n",
    "\n",
    "# Generate a color gradient across the sorted identifiers to color proteins with alphabetically similar names with similar colors.\n",
    "cmap = plt.get_cmap(\"viridis\")\n",
    "color_gradient = [cmap(i / (len(sorted_identifiers) - 1)) for i in range(len(sorted_identifiers))]\n",
    "color_dict = {identifier: f'rgba({int(r*255)}, {int(g*255)}, {int(b*255)}, {a})' for identifier, (r, g, b, a) in zip(sorted_identifiers, color_gradient)}\n",
    "\n",
    "for idx, key in enumerate(subplot_titles):\n",
    "\n",
    "    scatter = px.scatter(\n",
    "        x=umap_results[key][:, 0], \n",
    "        y=umap_results[key][:, 1],\n",
    "        color=merged_df['Identifier'],\n",
    "        color_discrete_map=color_dict,\n",
    "        hover_name=merged_df['Identifier'],\n",
    "        labels={'x': 'UMAP 1', 'y': 'UMAP 2'}\n",
    "    )\n",
    "    scatter.update_traces(marker=dict(size=4))\n",
    "    \n",
    "    row = (idx // 2) + 1\n",
    "    col = (idx % 2) + 1\n",
    "    \n",
    "    for trace in scatter['data']:\n",
    "        trace.showlegend = False\n",
    "        fig.add_trace(trace, row=row, col=col)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800, \n",
    "    width=800,\n",
    "    title_text=\"UMAP Projections: Zeros, Mean, Small Latent (512), and Large Latent (768)\",\n",
    "    dragmode='zoom'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
